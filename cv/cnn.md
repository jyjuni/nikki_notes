### Batch Normalization

$$

$$



### Softmax

approximation for max so that it has a gradient
$$

$$

### Dropout

with (1-p) probability it puts 0, otherwise propagates.
$$

$$

### Width vs Depth

**Universal approximation theorem**: with sufficiently wide network and just one layer, you can appoximate any continuous function with arbitrary approximation error.

