# Syntax and Formal Languages

[Slide #6]() [lecture06-syntax_formal_languages.pdf](../slides/lecture06-syntax_formal_languages.pdf) 

## Syntax vs. semantic

* can tell grammatical vs. ungrammatical
* even if it doesn’t make sense semantically
* use * to denote ungrammatical expressions

## Liguistic theories

* prescriptive
* **descriptive**
* explanatory

## Syntax

### Overview

* constituency and recursion(NP inside NP)

* dependency(grammatical dependencies between words, eg: subject-object)

* grammatical relations

* subcategorization 

  > eg: verb + pp. vs verb w/t pp.:
  >
  > I **_want_** to fly to New York
  >
  > I **_found_** to fly to New York  

* Long-distance dependencies

### Constituents
**a group of words that behave as a single unit**

constituency test:

* Topicalization
	* **_atomic_** unit that can move around, but cannot be breaking apart

* Pro-form substitution: replace with pronoun
* Questiontests

#### Sentence Structure as Trees

* Constituent Labels

  * Choose constituents so each one has one *non-bracketed* word: the **head.**

  * Category of Constituent: **XP**, where X is the part-of-speech of the head 

    > NP, VP, AdjP, AdvP, DetP 

* We can obtain a dependency structure by projecting the heads up the tree.

* phrase structure and dependency structure.

  <img src="NLP 0713.assets/Screen Shot 2022-08-09 at 8.39.24 PM.png" alt="Screen Shot 2022-08-09 at 8.39.24 PM" style="zoom:50%;" />

### Recursion

> [The mouse [the cat [the dog chased]] ate] died.  
How do we model the set of sentences in a language and their structure?

### Context Free Grammars(CFG)
A CFG is defined by:

- Set of **terminal symbols** *Σ*.
- Set of **non-terminal symbols** *N*. 
- A **start symbol** *S* ∈ *N.*
- Set *R* of **productions** of the form *A* *→ β*,   terminals and non-terminals. where A ∈ N and *β* ∈ *(Σ* ∪ *N)* *, i.e. *β* is a string of terminals and non-terminals.

#### Language of a CFG

* $ \alpha \to  \beta$ means that G can derive $\beta$ from $\alpha$ in a step

* $ \alpha \to  ^*\beta$ means that G can derive $\beta$ from $\alpha$ in a finite number of steps

* **Language of a CFG:** set of all terminal strings that can be derived from the start symbol

$$
L(G) = \{{\beta \in T^\ast s.t. S \rightarrow  ^\ast \beta}\}
$$

#### Derivations and Derived Strings

* CFG is a string rewriting formalism, so **derived objects** are strings.
* A **derivation** is a sequence of rewriting steps
* **context free**: applicability of a rule depends only on the nonterminal symbol, not on its context.

#### Regular Grammars

A regular grammar is defined by:

- Set of **terminal symbols** *Σ*.
- Set of **non-terminal symbols** *N*. 
- A **start symbol** *S* ∈ *N.*
- Set *R* of **productions** of the form *A* *→ aB*, ir *A→ a*  where A,B ∈ N and *a ∈ Σ*

#### Finite State Automata
The set of all regular languages is strictly smaller than the set of context-free languages.

_Natural languages (such as English, specifically) are _**_NOT_**_ regular_

> Example: Center embeddings, Crossing Dependencies  

#### Center Embeddings

**problem**: regular grammars cannot capture long-dostance dependencies

**form**: $aaa...bbb = a^nb^n$. Can show that is language is ***not* context-free** 

<img src="NLP 0713.assets/Screen Shot 2022-08-09 at 8.54.01 PM.png" alt="Screen Shot 2022-08-09 at 8.54.01 PM" style="zoom:30%;" />

#### Crossing Dependencies
Context Free Grammar **CANNOT** describe crossing dependencies

#### Complexity Classes(Chomsky Hierarchy)

<img src="NLP 0713.assets/7090.1571152901.jpg" alt="Chomsky Hierarchy" style="zoom:33%;" />



# Parsing with CFGs

## Two approaches:
* bottom-up
	* start at the words(terminal symbols) and see which subtrees you can build.
	
	  eg: CKY Algorithm
	
* top-down
	* start at the start symbol S, try to apply production rules that are compatible with the input.
	
	* (recursively replace a production rule with corresponding production rules)
	
	  eg: Earley Algorithm
	

----



# The CKY Algorithm

[lecture07_cfg_parsing_cky.pdf](../slides/lecture07_cfg_parsing_cky.pdf) 

## Chomsky Normal Form

all rules in one of the two forms:

* $A \to B C$, where $A,B,C \in N$
* $A \to b$, where $A \in N$, $b \in \Sigma$

> Any CFG can be converted to an equivalent grammar in CNF that expresses the same language.

## CKY algorithm

* approach: compute all $\pi[i,j]$ for all sub-spans bottom-up, using dynamic programming

* spans to form the final result may not overlap

* try all possible split points $k:$  $i<k<j, \ s.t. \ span[i,j] = span[i,k]+span[k,j]$

* for each k, check if the nonterminals in $\pi[i,k]$ and $\pi[k,j]$ match any of the rules in the grammar.

* recursive definition for $\pi[i,j]$: 

  $\pi[i,j] = \displaystyle\cup_{k=i+1...j-1} \{A \vert  A \to BC \in R \ and \ B \in \pi[i,k] \ and \ C \in \pi[k,j] \} $
  <img src="NLP 0713.assets/Screen Shot 2022-07-24 at 5.47.12 PM.png" alt="Screen Shot 2022-07-24 at 5.47.12 PM" style="zoom: 33%;" />

<img src="NLP 0713.assets/Screen Shot 2022-08-09 at 9.18.23 PM.png" alt="Screen Shot 2022-08-09 at 9.18.23 PM" style="zoom:40%;" />

 [Ungraded Exrecise: CKY parsing](https://courseworks2.columbia.edu/courses/153361/assignments/865020?module_item_id=1794575) 

### Syntactic Ambiguities

_**Backpointers**_
* instead of storing a set of nonterminals, store a list of instantiated rules and backpointers.

  <img src="NLP 0713.assets/Screen Shot 2022-08-09 at 9.20.13 PM.png" alt="Screen Shot 2022-08-09 at 9.20.13 PM" style="zoom:50%;" />
  <img src="NLP 0713.assets/Screen Shot 2022-08-09 at 9.19.15 PM.png" alt="Screen Shot 2022-08-09 at 9.19.15 PM" style="zoom:50%;" />

_**Retrieving Parse-trees**_

* retrieving all: worst case, exponantial
* retreiving top k highest-scoring tree: polynomial times O(N^k)?
* retrieving any single parse tree: O(N^2)

### Probabilities for Parse Tree

* we want a model that assigns a probability to each parse tree, and  $$\sum_{t \in T_G} P(T)=1$$
* can use this to select the most probable parse tree: $\displaystyle \arg\max_{t \in T_G}P(T)$
* How do we define $P(T)$?
#### Probability Context Free Grammars(PCFG)

A PCFG consists of a Context Free Grammar   G=(N, Σ, R, S) and a probability P(A → *β*) for each production A → *β* ∈ *R.*

* the probabilities for all rules with the same left-hand-side sum up to 1.
* Think of this as the conditional probability for $A \to \beta$ given A
* Example :

<img src="NLP 0713.assets/IMG_0438.jpeg" alt="IMG_0438" style="zoom:25%;" />

#### Parse Tree Probability

Given a parse Tree $t \in T_G$, containing rules $A_1 \to \beta_1, ..., A_n \to \beta_n$

The probability of $t$ is:	 $P(t) = \displaystyle \prod_{i=1}^n P(A_i \to \beta_i)$

<img src="NLP 0713.assets/Screen Shot 2022-08-09 at 9.25.09 PM.png" alt="Screen Shot 2022-08-09 at 9.25.09 PM" style="zoom:50%;" />



#### Estimating PCFG Probabilities
* supervised training: estimate PCFG prob from a _treebank_(using MLE)

​												$P(A \to \beta) = \frac{count(A \to \beta)}{count(A)}$  

* the Penn Treebank

#### Parsing with PCFG
<img src="NLP 0713.assets/image-20220724194210904.png" alt="image-20220724194210904" style="zoom:30%;" />

- Use PCFG to answer:

  * What is the **total probability** of the sentence under the PCFG
  - What is the **most probable parse tree** for a sentence under the PCFG? (decoding/parsing)

- we can modify the CKY algorithm:

> let $\pi[i,j,X]$ be the prob of the highest scoring parse tree for $s[i,j]$ starting in nonterminal X
> * same runtime as CKY(why?)  

#### Probability of a Sentence
* we are interested in the probability 

* need to sum the probability of **all** parse trees for this sentence

* modify CKY: use sum instead of max
  $$
  \pi[i,j,A] = \sum_{\substack{k=i+1...j-1\\{A->BC} \in R}}P(A\to BC) \cdot\pi[i,k,B]\cdot\pi[k,j,C]
  $$
