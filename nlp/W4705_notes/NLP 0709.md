# Naive Bayes’ Classifier and Text Classification

[lecture03-naive_bayes.pdf](../slides/lecture03-naive_bayes.pdf) 

## Text Classification

**Task**: Given a representation of some document $d$, identify which class $c \in C$ the document belongs to

<img src="NLP 0709.assets/Screen Shot 2022-08-09 at 3.24.36 PM.png" alt="Screen Shot 2022-08-09 at 3.24.36 PM" style="zoom:40%;" />

This is a Machine Learning problem:

- can use different ML techniques:

  - **supervised ML**: fixed set of classes $C$. 

    Train a classifier from a set of labeled $<document, class>$ pairs.

    - discriminative  vs. generative models

  - unsupervised ML: unknown set of classes $C$.

    Topic modeling

- How do we represent each document?  

  (feature representation).

### Supervised Learning

**Given**: Training data consisting of training examples   $(x_1, y_1), ..., (x_n, y_n)$, where $x_i$ is an input example (a $d$-dimensional vector of attribute values) and $y_i$ is the label.

**Goal**: learn a hypothesis function $h(x)$ that approximates the true relationship between $x$ and $y$. This functions should 

	1. ideally be consistent with the training data. 
	1. generalize to unseen examples.

- find $h(x) \mapsto   y$ that minimize:
  - ==(1) empirical risk (error rate)== 
  - ==(2) structual risk (expected)==

- In NLP $y_i$ typically form a finite, discrete set.

### Representing Documents

- set-of-words representation						
- bag-of-words representation (Multi-set)
- vector-space model
  - Each word corresponds to one dimension in vector space. 
  - <img src="NLP 0709.assets/Screen Shot 2022-08-09 at 3.36.47 PM.png" alt="Screen Shot 2022-08-09 at 3.36.47 PM" style="zoom:40%;" /><img src="NLP 0709.assets/Screen Shot 2022-08-09 at 3.38.20 PM.png" alt="Screen Shot 2022-08-09 at 3.38.20 PM" style="zoom:50%;" />
  - Entries are either:
    - binary
    - raw/normalized frequency counts
    - weighted frequency counts
    - probabilities

## Probability in NLP

- Ambiguity is everywhere in NLP. There is often *uncertainty* about the “correct” interpretation. Which is more likely:
  - speech recognition
  - machine translation
  - text classification

- Probabilities make it possible to combine evidence from multiple sources systematically to (using Bayesian statistics)

### Bayesian Statistics

- observe some evidence and the goal is to infer the "correct interpretation"

  >  eg: evidence - words in a document; interpretation - topic of a text

- probabilities express the degree of belief we have in the possible interpretations

  - ***prior* probabilities**: probability of an interpretation prior to seeing any evidence

  - **conditional(*posterior*) probability: **probability of an interpretation after taking evidence into account

    

### Probability Basics

**sample space: **$\Omega$. Each is a possible basic outcome / “possible word"

A **probability distribution** assigns a probability to each basic outcome.
$$
P(\omega) \le 1.0 \text { for every } \omega\in\Omega \\

\sum_{\omega\in\Omega} P(\omega) = 1.0
$$
An **event** A is any subset of $\Omega$. 
$$
P(A) = \sum_{\omega\in A} P(\omega)
$$
**random variables**

**Joint and Conditional Probability**

**Independence**



### Probabilities and Supervised Learning

**Given**: Training data consisting of training examples  data=$(x_1, y_1), ..., (x_n, y_n)$

**Goal**: Learn a mapping $h$ from $x$ to $y$.

We would like to learn this mapping using $P(y\vert x)$ .

#### Discriminative and Generative Approach

- Discriminative algorithms: learn $P(y\vert x)$  directly

- Generative algorithms: use Bayes rule.  
  $$
  P(y\vert x) = \frac{P(x\vert y)P(y)}{P(x)}
  $$

##### Discriminative Algorithms

- Model conditional distribution of the label given the data  $P(y\vert x)$
- **learning**: learns decision boundaries that separate instances of the different classes.

- **predict**: cheks on which side of the decision boundary it falls
- **Examples**: linear and log-linear models, SVM, decision trees, random forests, ...

##### Generative Algorithms

- Assume the observed data is being “*generated*” by a “*hidden*” class label.

- **learning**: Build a **different distribution** for each class: $P(x\vert y)$

- **predicting**: check it under each of the models and see which one maches best

- **Examples**:  Naive Bayes, Hidden Markov Models, Gaussian Mixture Models, PCFGs, ...

  

## Naive Bayes Classifier

**Independence Assumption**: $X_i$  conditionally independent

<img src="NLP 0709.assets/Screen Shot 2022-08-09 at 4.12.06 PM.png" alt="Screen Shot 2022-08-09 at 4.12.06 PM" style="zoom:50%;" />
$$
\begin{align}

P(Label, X_1,...X_d) & = P(Label) \prod_iP(X_i\vert Label) \\

P(Label\vert X_1,...X_d) & = \frac{P(Label) \prod_iP(X_i\vert Label) }{\prod_iP(X_i)} \\
& = \alpha[P(Label) \prod_iP(X_i\vert Label)] \\

\end{align}
$$
Note that the normalizer $α$ does no longer matter for the argmax because α is independent of the class label:
$$
y^*  = \arg\max_y P(y) \prod_iP(x_i\vert y)
$$

### Training

**Goal**: Use the training data to estimate $P(Label)$ and $P(X_i\vert Label)$ from training data. 

Estimate the prior and posterior using MLE:
$$
P(y) = \frac{Count(y)}{\sum_{y' \in Y} Count(y')} \\
P(x_i\vert y) = \frac{Count(x_i, y)}{\sum_{x'} Count(x',y)}
$$
i.e. we just count how often each token in the document appears together with each class label.

> Example:  [Ungraded Excercise: Naive Bayes](https://courseworks2.columbia.edu/courses/153361/assignments/861935?module_item_id) 

#### Why the Independence Assumption Matters

- Without the independence assumption we would have to estimate $P(X_1,...X_d \vert  Label)$
- There would be many *combinations* of $X_1,...X_d$ that are never seen (**sparse** data).
- The independence assumption allows us to estimate each $P(X_i \vert  Label)$ independently.

#### Issues to consider

- unseen words
- words that appears only once in training set
- plural of a words unseen
- extremely common words
- **What is a word?**

#### Text Normalization

*包含tokenization, lemmatizationg, stemming, ...*

- every NLP tasks needs to do some text normalization
- sentence-level: **segmenting/tokenizing** words in running text
- word-level: normalizing word forms(lemmatization/stemming, possibly replacing named-entities)
- sentence splitting

## Linguistic Terminology

**Sentence**: Unit of written language.

**Utterance话语**: Unit of spoken language.

Word **Form:** the inflected form as it actually appears in the corpus. 

>  *“produced”*

Word **Stem**: The part of the word that never changes between morphological variations. 

>   *“produc”*

**==Lemma==**: an *abstract* base form, shared by word forms, having the same stem, part of speech, and word sense – stands for the **class** of words with stem.

> *“produce”*

**Type**: number of **distinct** words in a corpus (vocabulary size).

**Token**: Total number of word occurrences**.** (number of **intances**)

### Tokenization

The process of segmenting text (a sequence of characters) into a sequence of tokens (words).

<img src="NLP 0709.assets/Screen Shot 2022-08-09 at 4.31.38 PM.png" alt="Screen Shot 2022-08-09 at 4.31.38 PM" style="zoom:50%;" />

- Simple (but weak) approach: Separate off punctuation. Then split on whitespaces.
- Typical implementations use regular expressions (finite state automata).

issues: 

- dealing with punctuation 
  - some may be part of a word: “Ph.D.”, “O’Reilly”, “pick-me-up”
- which tokens to include:
  - punctuation might be useful for parsing, but not for text classification

- language dependent: 

  - Some languages don’t separate words with whitespaces.  

    > de: *“Lebensversicherungsgesellschaftsangestellter”* 

### Lemmatization

Converting Lemmas into their base form.

<img src="NLP 0709.assets/Screen Shot 2022-08-09 at 4.31.26 PM.png" alt="Screen Shot 2022-08-09 at 4.31.26 PM" style="zoom:50%;" />

## N-gram Language Models

 [lecture04-ngram_lang_models.pdf](../slides/lecture04-ngram_lang_models.pdf) 

## Language Modeling

**Task**: **predict the next word** given the context.

Can also be used to describe the **probability of an entire sentence**.

Used in: speech recognition, handwritten character recognition, spelling correction, text entry UI, machine translation,...

### Probability of the Next Word

- Idea: We do not need to model domain, syntactic, and lexical knowledge perfectly.

- Instead, we can rely on the notion of **probability of a sequence** (letters, words...).
  $$
  P(w_n\vert w_1, w_2, ..., w_{n-1})
  $$
  

### Probability of an entire sentence

This model can also be used to describe the probability of an entire sentence, not just the last word.

Use the chain rule:
$$
\begin{align}
P(w_1, w_2, ..., w_{n}) = &
P(w_1\vert w_2, w_2, ..., w_n)P(w_2, ..., w_{n}) \\
= & P(w_1\vert w_2, w_2, ..., w_n)P(w_2\vert w_3, ..., w_{n}) 
\end{align}
$$

## Markov Assumption

$P(w_n\vert w_1, w_2, ..., w_{n-1})$ is difficult to estimate

The longer the sequence becomes, the less likely $w_1, w_2, ..., w_{n-1}$ will appear in training data.

Intead, we make the following simple independence assumption (**Markov assumption**):

The probability to see $w_n$ depends only on the previous $k-1$ words: 
$$
P(w_n\vert w_1, w_2, ..., w_{n-1}) \approx P(w_n\vert w_{n-k+1}, ..., w_{n-1})
$$

## N-gram Language Model

### n-grams

The sequence $w_n$ is a unigram. 

The sequence $w_{n-1}, w_n$ is a bigram.

The sequence $w_{n-2}, w_{n-1}, w_n$ is a trigram.... 

The sequence $w_{n-3}, w_{n-2}, w_{n-1}, w_n$ is a quadrigram...

### bi-gram Language model

k=2: 
$$
P(w_1, w_2, ..., w_{n}) \approx P(w1) \cdot P(w_2\vert w_1) \cdot P(w_3\vert w_2) \cdot\cdot\cdot P(w_n\vert w_{n-1}) \\
$$
more consistent to use only bigrams: 
$$
P(w_1, w_2, ..., w_{n}) \approx P(w1\vert start) \cdot P(w_2\vert w_1) \cdot P(w_3\vert w_2) \cdot\cdot\cdot P(w_n\vert w_{n-1}) \\
$$


### Variable-Length Language Models

We typically don’t know what the length of the sentence is.

Instead, we use a special marker STOP/END that indicates the end of a sentence.

We typically just augment the sentence with START and STOP markers to provide the appropriate context.

> *START i want to eat Chinese food END*
> $$
> \text{bigram:} \\
> P(i\vert START)·P(want\vert i)·P(to\vert want)·P(eat\vert to)·\\
> P(Chinese\vert eat)·P(food\vert Chinese)·P(END\vert food)\\
> \\
> \text{trigram:} \\
> 
> P(i\vert START, START)·P(want\vert START,i)·P(to\vert i,want)·P(eat\vert want,to)·\\
> P(Chinese\vert to,eat) · P(food\vert eat,Chinese)·P(END\vert Chinese,food)
> $$

### log probabilities

Probabilities can become very small (a few orders of magnitude per token).

We often work with log probabilities in practice.
$$
p(w_1,...,w_n) = \prod_{i=1}^{n}p(w_i\vert w_{i-1}) \\
\log p(w_1,...,w_n) = \sum_{i=1}^{n} \log p(w_i\vert w_{i-1})
$$

### What do ngrams capture?

Probabilities seem to capture *syntactic facts* and  *world knowledge.*

> *eat* is often followed by a NP.
>
> *British* food is not too popular, but *Chinese* is.

### Estimating n-gram probabilities

We can estimate n-gram probabilities using maximum likelihood estimates.

for **bigrams**:
$$
p(w\vert u) = \frac{count(u,w)}{ count(u)}
$$
Or for **trigrams**:
$$
p(w\vert u,v) = \frac{count(u,w,v)}{ count(u,v)}
$$


### Corpora

- Large digital collections of text or speech. Different languages, domains, modalities. Annotated or un-annontated.
- English:
  -  Brown Corpus
  - BNC, ANC
  - Wall Street Journal
  - AP newswire
  - Gigaword, WAC, ...
  - DARPA/NIST text/speech corpora 
     (Call Home,ATIS, switchboard, Broadcast News,...)
  - **MT**: Hansards, Europarl

- Google Web 1T 5-gram Corpus

  >  <img src="NLP 0709.assets/Screen Shot 2022-08-09 at 5.25.47 PM.png" alt="Screen Shot 2022-08-09 at 5.25.47 PM" style="zoom:30%;" />