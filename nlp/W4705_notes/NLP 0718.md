review for [CKY Algorithms](../slides/lecture07_cfg_parsing_cky.pdf)

# Earley Algorithm

[Slides #08](../slides/lecture08-earley_algorithm.pdf)

## Two Approaches to Parsing

**Bottom-up**: start at the words(terminals), see which subtrees you can build.

> eg: CKY Algorithm

**Top-down**: start at tge start symbol(S), try to apply productions compatible with the input.

> eg: Earley algorithm



## Earley Parser

- It discards derivations that are incompatible with the sentence.
- Allows arbitrary CFGs.: does not requires the grammar to be in CNF (CKY requires)

### Parser States

- keeps track of partial derivations using parser

  **states / items**.

- states represent **hypotheses**

- parser states are represented as **dotted rules with spans**

  - *left* of the · : have already been seen in the input, corresponding to the span

    > S → · NP VP [0,0]
    >
    > NP → D A · N [0,2]
    >
    > NP → NP PP · [3,8]

- parse chart organized by end-position

### Algorithm

<img src="NLP 0718.assets/Screen Shot 2022-07-18 at 5.09.47 PM.png" alt="Screen Shot 2022-07-18 at 5.09.47 PM" style="zoom:30%;" />

***shift/scan***: proceed

can only be applied to a state if the dot is   in front of a terminal symbol that matches the next input   terminal.

>  function **shift**(state): 		// state is of form $A \to \alpha · s[i] \  \beta \ [k,i]$
> 		Add a new state$A \to \alpha \ s[i] ·   \beta \ [k,i+1]$ to $Chart[i+1]$

**predict**: hypothesis

can only be applied to a state if the dot is   in front of a non-terminal symbol.

> function **predict(**state): 		//state  is of form $A \to \alpha · B \  \beta \ [k,i]$
>
> ​		For each rule $B \to \gamma \in R$:
>
> ​				Add a new state $B \to \cdot \ \gamma \ [i,i]$ *to* $Chart[i]$

***complete***: combine(wrap up previous states awaits)

may only be applied to a passive item.

- similar to the combination operation in CKY

>  function **complete(**state): 	// *state* is of form $A \to \alpha ·  [k,i]$
>
> ​		for each state $B \to \beta \cdot A \  \gamma \ [j,k]$: //state:end with k(i.e. in chart k) AND dot right is A
>
> ​				add a new state  $B \to \beta \ A \cdot \gamma [j,i]$ to $Chart[i]$

> **Example**
>
> <img src="NLP 0718.assets/Screen Shot 2022-08-09 at 10.28.31 PM.png" alt="Screen Shot 2022-08-09 at 10.28.31 PM" style="zoom:45%;" />
>
> 
>
> <img src="NLP 0718.assets/Screen Shot 2022-08-09 at 10.29.19 PM.png" alt="Screen Shot 2022-08-09 at 10.29.19 PM" style="zoom:45%;" />
>
> <img src="NLP 0718.assets/image-20220810115525781.png" alt="image-20220810115525781" style="zoom:50%;" />

- What happens in case of ambiguity?
  - Multiple ways to Complete the same state.
- recover parse trees:
  - keep back pointers in the parser state objects
- work with PCFG:
  - compute probabilities on complete
  - follow back pointer with max prob
- complexity: O(N^3), assume constant grammar rules 



# Dependency Parsing

[Slide #09](../slides/lecture09-dependency_parsing.pdf)

## Dependency Structure

*<img src="NLP 0718.assets/Screen Shot 2022-08-10 at 12.41.04 AM.png" alt="Screen Shot 2022-08-10 at 12.41.04 AM" style="zoom: 33%;" />

The*edges* labeled with **grammatical relations** between words

- arguments(subject, object, indirect object, prepositional object)
- adjunct(temporal, locative, causal, manner)/modifier
- function words

### Dependency Relations

- head + dependent (always head -> dependent)

  <img src="NLP 0718.assets/Screen Shot 2022-08-10 at 12.39.21 AM.png" alt="Screen Shot 2022-08-10 at 12.39.21 AM" style="zoom:50%;" />

- represent:

  > subj*(likes-02, girl-01)* *or *(likes, nsubj, girl)*
  >
  > <u>entire sentence:</u>
  >
  > *root(likes-2), subj(likes-2, girl-1), det(the-0, girl-1), obj(likes-2, boys-7),  det(boys-7, few-4), det(few-4, a-3), amod(boys-7, friendly-6), advmod(friendly-6, very-5)*

- Different notations/graphs

- Different Representations

  <img src="NLP 0718.assets/Screen Shot 2022-07-18 at 6.27.58 PM.png" alt="Screen Shot 2022-07-18 at 6.27.58 PM" style="zoom: 45%;" />

### Dependency Tree

Dependency structure is typically assumed to be a tree:

- root no parent
- all other nodes have exactly 1 parent
- connected, no cycle

### Projectivity

- words in a sentence stand in linear order
- **edges cannot cross**
- some appraoch in depenency parsing *cannot* handle non-projectivity
- An edge (i, r, j) in a dependency tree is projective if there is a   directed path from i to k for all i < k < j (if i < j) or j < k < i (j < i).

## Dependency Parsing

### Setup

**Input**:

- $V_s = \{ w_0, w_1, ..., w_m\}$ - set of nodes corresponding to the input sentence  $s = w_1, ..., w_m$
- $R$ - set of labels = $\{PRED, SBJ, OBJ, ATT\}$

**Goal**:

- find a set of labeled, directed edges between the nodes such that the resulting graph forms a **correct dependency trees** over $V_s$

**Task:**

As with other NLP problems, we can think of dependency parsing as a kind of search problem:

- Step 1: Define the space of possible analyses for a sentence
- Step 2: Select the best analysis from this search space.

Need to define the search space, search algorithm, and a way to determine the "best" parse.

### Subcategorization/Valency

- Verbs takes different **number of arguments**

  > - The baby slept. 	\* The baby slept the house.
  > - He pretended to sleep. 	\*He pretended the cat.
  > - Godzilla destroyed the city. 	\*Godzilla destroyed.
  > - Jenny gave the book to Carl. 	\*Jenny gave the book.

### Transition-Based Approach

- Start with an initial configuration and find a sequence of transitions to the terminal state.

- Uses a greedy approach to find the best sequence of transitions.
  - Uses a discriminative model (classifier) to select the next transition.

A parser state (configuration) is a triple c = (σ,β,A)

- $\sigma$ is a **stack** of words $wi ∈ Vs$
- $\beta$ is a **buffer** of words $wi ∈ Vs$
- A - is a set of dependency arcs $(w_i, r, w_j)$

#### States

**initial state:** 

$c_0: \ \Big([w_0]_{\sigma}, \ [w_1 , w_2 , ... ,w_m ]_{\beta} , \ \{\}_A \Big)$

**terminal state:**

$c_t: \Big([w_0]_{\sigma}, \ [ ]_{\beta} , \ A)$  for any $\sigma$ and $A$

#### Transitions(Arc-Standard Transitions)

<img src="NLP 0718.assets/Screen Shot 2022-07-18 at 6.48.26 PM.png" alt="Screen Shot 2022-07-18 at 6.48.26 PM" style="zoom:50%;" />

- **shift:** Move next word from the buffer to the stack 

  $(\sigma , w_i \vert  \beta, A)  \to  (\sigma \vert  w_i, \beta, A)$

- **left-arc: ** Build an edge from the next word on the buffer to the top  word on the stack.
  
  $(\sigma\vert w_i , w_j \vert  \beta, A)  \to  (\sigma, w_j\vert \beta, A \cup \{w_j, r, w_i\})$

- **right-arc: **Build an edge from the top word on the stack to the next word on the buffer. 

  $(\sigma\vert w_i , w_j \vert  \beta, A)  \to  (\sigma, w_i\vert \beta, A \cup \{w_j, r, w_i\})$

#### Example

Oracle Example:

- assume we always know the next transition from Oracle

#### Properties of the transition system

- no dead end
  - will always reach to terminal state, because it can always shift
- predict the next transition: O(m) for m words
  - better than O(m^3) for CKY
- **bottom-up** approach: 
  - a node correct all its children before its parent
  - reason: if it become dependent, it will be removed next
- can only produce **projective** trees
  - reason: can only deal with dependency with immediate left/right node
- soundness
  - All terminal structures are projective forests

#### Deciding the next transition

use discrimitive models to model the probability distribution of transition operation conditioned on the current state

**input:** current state (stack and buffer)

**output:** current best transition operation.

In HW3, we used fixed size input for stack and buffer. What modification can you make to the network to handle various length input?

Use RNN (or attention) to encode the stack and buffer separately, and concatenate the encoding  as the representation of current state before feeding into further dense layers.

- if classifier takes O(1), O(m) for m words
- Question: what **feature** could the classifier use?

#### Features

- need to define a feature function: states -> feature

- each feature contains: 

  1. address in the state description

     >  (eg: top of stack)


  2. attribute

     > (eg: POS, word form, lemma, word embedding, ...)


Example Features([reading]())

#### Training the model

- data: Manually annotated (dependency) treebank

- problem: We have not actually seen the transition sequence, only the dependency trees!

- idea: Construct oracle **transition sequences** from the dependency tree. Train the model on these transitions. 

  <img src="NLP 0718.assets/Screen Shot 2022-08-10 at 1.14.18 AM.png" alt="Screen Shot 2022-08-10 at 1.14.18 AM" style="zoom:50%;" />

#### Arc-Eager Transitions

difference:

- **Right-arc**: 
  - Build an edge from the top word on the stack to the next word on the buffer. 

- **reduce:**
  - Remove a completed node from the stack.  

Exercise: [Transition-based dependency parsing](https://courseworks2.columbia.edu/courses/153361/assignments/868074)



### Graph-Based Approach

- <u>not</u> limited to projective dependency structure

- idea: use graph algorithms to compute a Maximum Spanning Tree(MST)

- general idea: each word needs to get a head, start with completely connected graph, score each edge, keep the highest scoring edges

- computing: Chu-Liu-Edmonds Algorithms for directed graphs

  
