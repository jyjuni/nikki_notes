## N-gram (continued)

 [lecture04-ngram_lang_models.pdf](../slides/lecture04-ngram_lang_models.pdf) 

### Data Sparsity

*test* data contains language phenomena not encountered during train time

two issues for n-grams:

- unseen tokens

- unseen n-grams

  - even though the individual tokens are known

  - token has not been encountered in this **context** before.

    > P(lunch \vert  I)=0.0

#### unseen words 

Typical approach:

- start with a specific lexicon of known tokens

- replace all tokens in **training and testing **corpus not in the lexicon with **UNK** token

- practical approach:

  - lexicon contains all words that appears >= k times (eg: more than once)

  - replace all other tokens with UNK

#### unseen contexts

> count(u,v,w)=0 
>
> what if count(u,v)=0: p(w\vert u,v)? 
>
> => set p(w\vert u,v) = 1/\vert v\vert  for all w, **CANNOT set to 0**

2 basic approaches:

- smoothing/discounting
  - move some probability mass from seen to unseen
  
- back-off
  - use shorter context(n-1, n-2 grams etc) to approximate n gram
  
    >  eg: use p(w\vert v) to approx. p(w\vert u,v)

**Zipf’s Law**:

a word’s frequency is aound inversely proportional to its rank in the word distribution list.

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 5.46.05 PM.png" alt="Screen Shot 2022-08-09 at 5.46.05 PM" style="zoom:50%;" />

#### Smoothing 	

flattens spiky distribution: "劫富济贫"

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 5.47.36 PM.png" alt="Screen Shot 2022-08-09 at 5.47.36 PM" style="zoom:50%;" />

##### Additive Smoothing

- classic approach: Laplacian, add one to every word count
$$
P(w_i) = \frac{count(w_i)+1}{N+V}
$$
- *N* - number of tokens, *V* - number of types (i.e. size of the vocabulary)

- Inaccurate in practice.

##### Linear interpolation

- use denser distribution of shorter ngrams to “fill in” sparse ngram distributions.
  $$
  p(w\vert u,v)=\lambda_1 \cdot p_{mle}(w\vert u,v)+\lambda_2 \cdot p_{mle}(w\vert v)+\lambda_3 \cdot p_{mle}(w) \\
  
  where \  \lambda_1, \lambda_2,\lambda_3 > 0 \ \ and \ \ \lambda_1+ \lambda_2+\lambda_3 = 1
  $$

- Works well in practice (but not a lot of theoretical justification why).

#### Discounting

- set aside some probability mass, then fill in the missing mass using back-off $$count^*(v,w) = count(v,w) - \beta, \ \  0<\beta<1$$

- Then all all *seen* bigram : $$p(w\vert v) = \frac {count^*(v,w)}{count(v)}$$
- For each context $v$, the missiing probability mass is $$ \alpha(v) = 1 - \sum_{w:c(v,w)>0}\frac {count^*(v,w)}{count(v)}$$
- We can now divide this held-out mass between the unseen words(evenly or using back-off)

#### Backoff

##### Katz’s Backoff

Divide the held-out probability mass proportionally to the unigram probability of the unseen words in context $v$
$$
p(w\vert v) = 
\begin{cases}
\frac {count^*(v,w)}{count(v)}, & \text{if}\ count(v,w)>0 \\
\alpha(v) \times \frac {p_{mle}(w)}{\displaystyle \sum_{w':count(v,w')=0}p_{mle}(w')}, & \text{otherwise.}

\end{cases}
$$


For trigrams: recursively compute backoff-probability for unseen bigrams. Then distribute the held-out probability mass proportionally to that bigram backoff-probability.


$$
p(w\vert u, v) = 
\begin{cases}
\frac {count^*(u,v,w)}{count(u,v)}, & \text{if}\ count(u,v,w)>0 \\
\alpha(u,v) \times \frac {p_{BO}(w\vert v)}{\displaystyle \sum_{z:count(u,v,w)=0}p_{BO}(z\vert v)}, & \text{otherwise.}

\end{cases}
$$
where: $\alpha(u,v) = 1 - \displaystyle \sum_{w:c(u,v,w)>0}\frac {count^*(u,v,w)}{count(u,v)}$

- often combine with Good-Turing smoothing

### Evaluation

- extrinsic evaluation
  - Apply the model in an application (for example language classification). Evaluate the application.

- intrinsic evaluation
  - measure how well the model approximates unseen language data.
  - typically use *perplexity* instead


#### Perplexity

perplexity(per word) measures how well the ngram model predicts the sample

defined as: $2^{-l}$, where $l = \frac 1 M \displaystyle \sum_{i=1}^m \log_2 p(s_i)$

- measure of surprise, lower=better
- intuition:
  - assume we are predicting one word at a time
  - With uniform distribution, all successor words are equally likely. Perplexity is equal to vocabulary size.
  - can be thought of as **effective vocabulary size**
- 100~300 is typically good

### vs. Naive Bayes 

no longer using independent assumption (thus less naive) as in Naive-Bayes



# Sequence Labeling (POS Tagging)

 [lecture05-hmm_pos_tagging.pdf](../slides/lecture05-hmm_pos_tagging.pdf) 

## Garden-Path Sentences

> The horse raced past the barn fell

- raced: past tense verb/past participle (passive voice).   

- The verb interpretation is more likely before *fell* is read.

- Once *fell* is read, the verb interpretation is impossible.

>  The old <u>**dog**</u> the footsteps of the young

- dog:noun/verb



## Parts-of-Speech

Classes of words that behave alike:

- similar context
- similar grammatical function
- similar morphological function (-es, -s, -ed...)
- similar meaning

#### ~9 traditional part-of-speech

- noun, pronoun, determiner, adjective, verb, adverb, preposition, conjunction, interjection

#### Function of P.O.S

interacts with most levels of linguistic representation

- speech processing:

  > lead(n.) vs. lead(v.)
  >
  > **in**sult, in**sult**
  >
  > **ob**ject, ob**ject**
  >
  > con**tent**, content

- Syntactic parsing
- P.O.S. tag-set should contain **morphological** and maybe **syntactic** information.

#### P.O.S. Tagset

- Penn Treebank Tagset

  <img src="NLP 0711.assets/Screen Shot 2022-08-09 at 6.36.03 PM.png" alt="Screen Shot 2022-08-09 at 6.36.03 PM" style="zoom:50%;" />

- tagest is language specific
  - Some language capture more morphological information which should be reflected in the tag set.

- “Universal Part Of Speech Tags?”
  - *Petrov et al. 2011: Mapping of 25 language specific tag-sets to a common set of 12 universal tags*



## Part-of-Speech Tagging

Goal: Assign a part-of-speech label to each word in a sentence.

- an example of **sequence labeling**
- think of this as an **translation** task from a sequence of *words* (w1...wn) to a sequence of tags (t1...tn)

### Determining PoS

- syntatic tests - 句法

  > *A blue seat / A child seat:* noun or adj?
  >
  > *A very* **blue** *seat*		  	*\*A very **child** seat*.
  >
  > *This seat is* **blue** 			*\*This seat is **child.***

- morphological tests - 变形

  > *bluer* 						\**childer*

### Part-of-Speech Tagging Task

Task: **translate** from a sequence of *words* (w1...wn) to a sequence of tags (t1...tn)

- NLP is full of translation problems from one structure to another. Basic solution:
  - For each translation step:
    1. Construct search space of possible translations.
    2. Find best paths through this space (decoding) according to some performance measure.

### Bayesian Inference for Sequence Labeling

- Recall Bayesian Inference (Generative Models): Given some observation, infer the value of some *hidden variable.* (see Naive Bayes’)
- apply this apporach to sequence labeling
  - Assume each word wi in the observed sequence  (w1, w2, ..., wn) ∈ V* was generated by some hidden variable ti.
  - Infer the most likely sequence of hidden variables given the sequence of observed words.

##### Noisy Channel Model

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 6.49.49 PM.png" alt="Screen Shot 2022-08-09 at 6.49.49 PM" style="zoom:50%;" />

**Goal**: figure out what the original input to the the channel was. Use Bayes’ rule:
$$
\arg \max_{tags} P(tags\vert words) = \arg\max_{tags} \frac{P(tags) P(words\vert tags)}{P(words)}
$$



## Hidden Markov Models(HMMs)

Generative (Bayesian) probability model.  

**Observations**: sequences of words. 

**Hidden states**: sequence of part-of-speech labels.

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 6.53.05 PM.png" alt="Screen Shot 2022-08-09 at 6.53.05 PM" style="zoom:50%;" />

- Hidden sequence is generated by an n-gram language model

*Hidden*: infer hidden tagset from observed tokens(eg: time flies like an arrow)

### Markov Chain

<img src="NLP 0711.assets/Screen Shot 2022-07-11 at 6.08.32 PM.png" alt="Screen Shot 2022-07-11 at 6.08.32 PM" style="zoom:25%;" />

- A **Markov chain** is a sequence of random variables X1, X2, ...

- The domain of these variables is a set of states.

- **Markov assumption:** Next state depends only on current state.

  ​					$P(X_{n+1}\vert X_1, X_2, ..., X_n) = P(X_{n+1}\vert X_n)$

- This is a special case of a weighted finite state automaton (WFSA).

**two types of probabilities:**

Transition Probability and Emission Probabilities

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 6.57.26 PM.png" alt="Screen Shot 2022-08-09 at 6.57.26 PM" style="zoom:50%;" />
$$
\begin{align}
& P(t_1, t_2, ..., t_n, w_1, w_2, ..., w_n) \\
= & P(t_1\vert start)P(w_1\vert t_1)P(t_2\vert t_1)P(w_2\vert t_2)P(t_3\vert t_2)\cdot\cdot\cdot P(t_n\vert t_{n-1})P(w_n\vert t_n) \\
= & \prod_{i=1}^nP(t_i\vert t_{i-1})P(w_i\vert t_i)
 \end{align}
$$


### Important Tasks on HMMs

1. **<u>Decoding</u>**: Given a sequence of words, find the *most likely* tag sequence

   (Viterbi algorithm)

2. **<u>Evaluation</u>**: Given a sequence of words, find total prob for this word sequence given an HMM

   (Forward algorithm)

3. **<u>Training</u>**: Estimate emission and transition probabilities from training data. 

   (MLE, Forward-Backward a.k.a Baum-Welch algorithm)

   

### Decoding

<img src="NLP 0711.assets/Screen Shot 2022-08-09 at 7.03.43 PM.png" alt="Screen Shot 2022-08-09 at 7.03.43 PM" style="zoom:50%;" />

**Goal**: Find the path with the highest total probability (given the words) 
$$
\arg\max_{t1,...,tn}\prod_{i=1}^{n}p(t_i\vert t_{i-1})p(w_i\vert t_i)
$$
#### Viterbi Algorithm

idea： *with markov assumption, only need $X_n$  for $X_{n+1} $*

emission probability: $P(w_i\vert t_i)$

transition probability $P(t_{i}\vert t_{i-1})$

*This suggests a **dynamic programming** algorithm.

<img src="NLP 0711.assets/Screen Shot 2022-07-11 at 6.30.11 PM.png" alt="Screen Shot 2022-07-11 at 6.30.11 PM" style="zoom:25;" />

<img src="NLP 0711.assets/Screen Shot 2022-07-11 at 6.31.43 PM.png" alt="Screen Shot 2022-07-11 at 6.31.43 PM" style="zoom:25%;" />



 [Ungraded Exercise: HMM - Viterbi & Forward Algorithm](https://courseworks2.columbia.edu/courses/153361/assignments/862541?module_item_id=1787378) 

#### Trigram Language Model

Instead of using a unigram context$P(t_{i}\vert t_{i-1})$, use a bigram context $P(t_{i}\vert t_{i-2}, t_{i-1})$

- think of this as having states that represent *pairs of tags*

  ​										$\displaystyle \prod_{i=1}^{n}P(t_{i}\vert t_{i-2}, t_{i-1}) P(w_i\vert t_i)$

- Need to handle data sparseness when estimating transition probabilities (for example using backoff or linear interpolation)

#### More POS tagging tricks

(slide pp.39)

- often useful in practice to add an end-of-sentence marker (just like we did for n-gram language models).
- replace words with “pseudo words” representing an entire class.

### Evaluating

#### HMMs as Language model

- We can also use an HMM as **language models** (language generation, MT, ...), i.e. **evaluate** probability for a given sentence.
  - vs n-gram: add gramartical evaluation


- Problem: There are many tag-sequences that could have generated w1, ... wn. 

- This is an example of **spurious ambiguity.**

- need to compute:
  $$
  \begin{align}
  P(w_1, ... ,w_n) & = \sum_{t1,...,tn} P(w_1, ... ,w_n,t_1, ... ,t_n) \\
  & = \sum_{t1,...,tn} \prod_{i=1}^{n}p(t_i\vert t_{i-1})p(w_i\vert t_i)
   \end{align}
  $$

#### Forward Algorithm

*same as Viterbi, replace max with sum*

<img src="NLP 0711.assets/Screen Shot 2022-07-11 at 7.09.45 PM.png" alt="Screen Shot 2022-07-11 at 7.09.45 PM" style="zoom:25%;" />

 [Ungraded Exercise: HMM - Viterbi & Forward Algorithm](https://courseworks2.columbia.edu/courses/153361/assignments/862541?module_item_id=1787378) 



## Name Entity Recognition as Sequence Labeling

Use 3 tags:

O-outside, B-begining, I-inside

>  <u>identification</u> <u>of</u> <u>tetronic</u> <u>acid</u> <u>in</u> ...	
>
> ​		 O			 O	    B		I	 O

- Other encodings are possible (for example, NE-type specific)

- This can also be used for other tasks such as phrase chunking and semantic role labeling.

----



# Syntax Formal Languages

 [lecture06-syntax_formal_languages.pdf](../slides/lecture06-syntax_formal_languages.pdf) 

The * means *ungrammatical sentence*

## Syntax

- Study of structure of language

  - how words are arranged in a sentence (word order) and the relationship between them

  - goal: relate surface form to semantics(meaning)

    

#### Linguistic Theories

- **Prescriptive**: this is how people ought to talk
- **Descriptive**: provide a formal account of **how** people talk
  - NLP focuses on this part
- **Explanatory**: explain why people talk in a certain way
  - (identify underlying cognitive, or neural mechanism)

