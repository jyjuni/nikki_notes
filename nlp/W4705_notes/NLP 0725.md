# Lexical Semantics: Word Embeddings

[lecture12-static_word_embeddings.pdf](../slides/lecture12-static_word_embeddings.pdf) 

## Neural Language Model

**naive idea:** use one-hot embedding to represent each word, then use a feed-forward neural network with softmax activation to model the probability.

**problem**: one-hot embeddings are <u>high demensionality</u>, <u>sparse</u>, and <u>many unseen combinations</u>.

**solution:** use lower-dimension word embedding for word reprensentations.

embedding matrixes(weights) are *shared*.

## Similarity and Relatedness

**similar**: interchangable

**related**: not interchangable, but comes from similar context/lexical fields.

## Single Word Representation

### One-Hot Vector

disadv: <u>high demensionality</u>, <u>sparse</u>, and <u>many unseen combinations</u>.

### Dense Representation

adv: can show closer similarity for closer-related words

[Brown clustering](https://en.wikipedia.org/wiki/Brown_clustering): use Hidden-Markov Model to group words and assign clusters, that are assumed to be semantically related by virtue of their having been embedded in similar contexts. 

**Two approaches:**

1. distributional semantics (count-based approaches)

   - Based in linguistic theory ("distributional hypothesis")
   
   
      - context is symbolic, dimensions are interprestable
   

2. distributed semantics

   - neural-network based
   
   
      - dimensions are not necessarily interpreatble
   

**Distributional Hypothesis**: The meaning of a word can be determined from the contexts it appears in.

**context of a word:** Neighboring words around the target word. During modeling, we can control the width of the slicing window.

## Similarity measure

### Co-occurence Matrix:

<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 5.10.11 PM-8783416.png" alt="Screen Shot 2022-07-25 at 5.10.11 PM" style="zoom:50%;" />

<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 5.11.03 PM.png" alt="Screen Shot 2022-07-25 at 5.11.03 PM" style="zoom:45%;" />

### Geometric Interpretation

**euclidean distance**: measures magnitude only

normalize $\vert \vert ·\vert \vert $ of vector, or:

**<u>cosine similarity</u>:** measures orientation+magnitude
$$
sim_{cos}(\mathbf x,\mathbf y) = \frac{\mathbf x · \mathbf y}{\vert \mathbf x\vert _2 · \vert \mathbf y\vert _2} = \frac{\sum_i x_i \cdot y_i}{\sqrt{\sum_i x_i^2}\sqrt{\sum_i y_i^2}}
$$


- ***direction*** matters more than the location

- more suitable for *dense* representation

  <img src="NLP 0725.assets/Screen Shot 2022-07-25 at 5.14.57 PM.png" alt="Screen Shot 2022-07-25 at 5.14.57 PM" style="zoom:50%;" />

### Generalized Jaccard Similarity

- more suitable for *sparse* representation<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 5.18.17 PM.png" alt="Screen Shot 2022-07-25 at 5.18.17 PM" style="zoom:50%;" />

### clutsering and semantic maps



### Semantic Similarity vs. Relatedness

- Semantic Relatedness (**syntagmatic relatedness**)
  - Words that occur nearby each other, but are not necessarily similar.
- Semantic Similarity (**paradigmatic relatedness**)
  - Can typically substitute one word for another.

### Effect of context size

- smaller window: closely related/interchangable
- larger window: more related words of similar context

## Term weighting

not all context terms are equally relevant

some appear too often, some are too rare

**one solution:** tf*idf

### TF*IDF

A measurement of weighting the co-occurence: How much does a word tell me about the word it co-occur with?

**idea:** The words that are informative(more co-occurence and less universal) get emphasized, while the words that are less(not) informative get deemphasized.

For DSM: 'document' = target word *d*.

**<u>term frequency</u>:** 

how often does the term $t$ appear in the context window of the target word?
$$
tf_{t,d} = count(d,t) \\
$$
**<u>inverse document frequency:</u>** 

for how many words does $t$ appear in the context window
$$
idf_{t,D} = \log \frac{\vert D\vert }{\vert \{d \in D, count(d,t) > 0\}\vert } \\
$$
**<u>tf*idf:</u>**
$$
tf*idf = tf_{t,d} \cdot idf_{t,D}
$$

## Learning Word Embeddings with Neural Networks

### Sparse vs. Dense Vectors

Full co-occurrence matrix is very big and contains a lot of 0 entries.

- Potentially inconvenient to store. Slow computation.
- Synonyms may still contain orthogonal dimensions, which are irrelevant.

**Word embeddings** are representations of words in a low- dimensional, dense vector space. There are two main approaches:

- matrix decomposition, such as SVD

  - basically a way of decomposing that matrix and then reasssembly one in which similarities between the individual target words are approximately maintained.

  >  eg: GloVe

- Learn distributed embeddings using neural networks. Minimal feature-engineering required.

  > eg: Word2Vec

#### two flavors for learning embeddings:

- **Skip-Gram mode**l: 

  Input is a single word.
  Goal: Predict a probability for each context word.

- **Continuous bag-of-words (BOW)**: 

  Input is a representation of the context window.

  Goal: Predict a probability for each target word.

(Word2Vec, Mikolov et al. 2013)

### Skip-Gram Model

**input**: a sinlge word in one-hot repr.

**output**: probability to see any single word as a context word

#### **Architecture**:

<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 6.24.57 PM.png" alt="Screen Shot 2022-07-25 at 6.24.57 PM" style="zoom:50%;" />

compute for each input word 
$$
\sum_{-c <= j<=+c; \ j\ne 0} \log p(w_{t+c}\vert w_t)
$$
objective:
$$
min \ -\frac{1}{T} \sum_{t}^{T} \sum_{-c <= j<=+c;j\ne 0} \log p(w_{t+c}\vert w_t)
$$
#### Negative Sampling

softmax is prohibitively expensive for large vocabularies

**solution**: Instead of exhaustively looking at all context words, randomly sample some subset:

- Sample **good pairs** (words near target word)

- Sample **bad pairs** (words far from target word)

**idea**: Maximizing the similarity of the words in the same context and minimizing it when they occur in different contexts.

Consider a set D of "correct" word context pairs (*w*,*c*) and a set D̅ of "incorrect" pairs. 

<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 6.36.44 PM.png" alt="Screen Shot 2022-07-25 at 6.36.44 PM" style="zoom:50%;" />

improvement: Use alternative objective to **improves training efficiency**, avoiding taking softmax over the entire vocabulary

==drawbacks==: (1) incorrect pairs are constructed randomlu, might include correct pairs	 (2) unseen combinations



### ==Continuous Bag-of-Words (CBOW)==

<img src="NLP 0725.assets/image-20220725183840012.png" alt="image-20220725183840012" style="zoom:50%;" />

### Word2Vec vs. CBOW: 

hard to tell which one works better in general.



## Analogy/Relational Similarity

#### Parallelogram Model

apple:apple::tree :: grape:grapes::vine

<img src="NLP 0725.assets/Screen Shot 2022-07-25 at 6.48.57 PM.png" alt="Screen Shot 2022-07-25 at 6.48.57 PM" style="zoom:50%;" />

#### Some caveats

the linear relationships are only approximate. Nearest neighbours are typically mophological (tomator:tomato::red :: potato:potato::potatoes) 

#### Embeddings Reflect Cultural Biases

embeddings capture culturalbiases

because literacies are written with biases

> male-> science, female->art
>
> male:programmer::female:homemaker
>
> African names -> unpleasant words
>
> European surnames -> pleasant words



# Lexical Semantics II: Word Senses and Lexical Relations 

[lecture13_wordnet_wsd.pdf](../slides/lecture13_wordnet_wsd.pdf) 

## Natural Language Semantics

- meaning represtantions bridge between linguistic input and extra-linguistic knowledge

### Word Senses

difference word senses  of the same word can denote different (more or less related) concepts

> *bank* (of a river) vs. *bank* (financial institution) vs. *bank* (storage facility)

**<u>Lexeme</u>**: paring of a particular word form with its senses

### **Relations**

- Homonymy

  - multiple unrelated concepts correspond to the same word form 

    >  bank

- Polysemy

  - multiple semantically related concepts correspond to the same word form

    > *wood* (material that trees are made of)  vs.   *wood* (a forested area)

- Metonomy

  - a subtype of polysemy

  - One aspect of a concept is used to refer to other aspects of a concept (or the concept itself).

    > BUILDING <-> ORGANIZATION
    >
    > ANIMAL <-> MEAT (the *chicken* was overcooked,  the *chicken* eats a worm)

- Zeugma

  - when a single word is used with two other parts of a sentence but must be understood differently (word sense) in relation to each.

    >  *Does United serve breakfast and JFK?*

- Synonyms

  - two lexemes refere to the same concepts

    > couch/sofa

  - Lexical Substitution

    - Two lexemes are synonyms if they can be substituted for each other in a sentence, such that the sentence retains its meaning (truth conditions).
    - Note that synonymy is not a relationship between words, but between lexemes.

- Antonyms

  - senses are opposites with respect to one specific feature, otherwise they are very similar

    > *dark / light* (level of luminosity)
    >
    > *short / long* (length)

- Hyponymy

  - One sense is a hyponym (or subordinate) of another sense if the first sense is more specific, denoting a subclass of the other. (IS-A relationship).

    > *dog* is a hyponym of *mammal*.

  -  inverse relation: **hypernymy**

- Meronymy

  - Part-whole relationship.

  - A meronym is a part of another concept.

    > *leg* is a meronym of *chair*.

  - inverse relation: **holonymy**  



## WordNet

a lexical database containing English word senses and their relations.

- Represents word sense as **synsets**, sets of lemmas that have synonymous lexemes in one context.

  > {*composition, paper, report, theme*} 
  >
  > {*newspaper, paper*}
  >
  > {*paper*}

- Version 3.1 Contains synonyms, antonyms, hypernyms,  
   (some) meronyms, and frequency information for about 
   117,000 nouns, 11,500 verbs, 22,000 adjectives, and 4,500 adverbs.

[Try WordNet online](http://wordnetweb.princeton.edu/perl/webwn)

