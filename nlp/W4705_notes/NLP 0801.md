# Attention

## Attention Mechanism

### Conditioned Generation with Attention

Conditioned Generator + 

each $y_j$ has a unique context vector $c_j$ - attention weight

### Obtain alignment weights

- compute $score(s_{j-1}, h_i)$ :

- compute alignment weights

  $$\alpha_{j,i} = \text{softmax}[\text{score} (s_{j-1}, 1), ..., \text{score}(s_{j-1}, N)] $$


### Attention as Lookup



<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 4.40.04 PM.png" alt="Screen Shot 2022-08-01 at 4.40.04 PM" style="zoom:50%;" />

think of $\alpha_{j,i}$  as a compatibility score between Query and each Key.

key and values are basically the same

# Transformers

**(Vaswani et al., 2017 "Attention is all you need")**

The **Transformer** is a non-recurrent architecture for sequence modeling and transduction that is based exclusively on attention.



<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 4.47.13 PM.png" alt="Screen Shot 2022-08-01 at 4.47.13 PM" style="zoom:50%;" />

predict one output at a time



## Architecture

### encoder layer

<img src="../../../Library/Application Support/typora-user-images/Screen Shot 2022-10-08 at 12.40.57 AM.png" alt="Screen Shot 2022-10-08 at 12.40.57 AM" style="zoom: 33%;" />



### decoder layer

masked: not allowed to look to the right(because are all 0’s)

<img src="NLP 0801.assets/Screen Shot 2022-10-08 at 12.41.50 AM.png" alt="Screen Shot 2022-10-08 at 12.41.50 AM" style="zoom:33%;" />

<img src="NLP 0801.assets/image-20220801165707167.png" alt="image-20220801165707167" style="zoom:50%;" />

### scaled dot product attention score



### multi-head attention



### two types of attentions

- self-attention in encoder and decoder, individually

- cross-attention between encoder and decoder



### positional encoding



# Transformer-based Models

## GPT

Just a fancy pretrained LM.

Model consists of stacked transformer  **decoder** layers.

- pre-trained using langugage modeling objective(unsupervised):
  - maximize $[formula]$
- then fine-tune on a specific supervised problem.

==difference between encoder and decoder==

### GPT fine tuning

choices of:

- continue to train the *entire* network based on the pretrained model
- train a linear model (+softmax) that uses the   last transformer layer (for the *last* position)   to predict *y.*

- it helps to keep training with Language Modeling task (objective LI) while doing finetuning task(objective l2) at the same time, so the model learns about its context better:

  <img src="NLP 0801.assets/image-20220801171930678.png" alt="image-20220801171930678" style="zoom:50%;" />

### Representing tasks as GPT Input

Many NLP tasks can be set up as classification problems.



<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 5.21.40 PM.png" alt="Screen Shot 2022-08-01 at 5.21.40 PM" style="zoom:50%;" />

==function of the EXTRACT token?==



## BERT

idea: need to look to the right

vs GPT:

- GPT's LM objective only takes left context into account (masked attention in transformer decoder layers)
  - particularly problematic for tasks that operate on multiple sentences (e.g. entailment).

solution: “masked langugae model” objective for pretraining

- allow use of bidirectional self-attention(i.e. transformer encoder layers)

Secondary objective: next sentence prediction.

> vs Word2Vec:
>
> similar to word2vec, but getting real-time representaiton of the MASKED word.	
>
> **<u>BERT vs GPT:</u>**
>
> - Encoder vs Decoder
> - Pre-training Objective
> - Bidirectional vs unidirectional
>
> **<u>RNN vs Transformer: </u>**
>
> - Sequential vs Parallel
> - Long term dependencies
> - Whether need pre-training







https://beta.openai.com/playground

# Fine-Tuning Language Models

## Zero-shot and Few-Shot Learning

**Zero-shot**

**One-shot**

**Few-shot**

<img src="NLP 0801.assets/image-20220801175002193.png" alt="image-20220801175002193" style="zoom:50%;" />

## Winograd Schemes

## Arithmetic



## PaLM

Chain-of-thought Prompting

<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 6.12.18 PM.png" alt="Screen Shot 2022-08-01 at 6.12.18 PM" style="zoom:50%;" />



# Word Embeddings: Summary

> <u>**Word representation/encoding w/ disadvantages**</u>
>
> Bag-of-words 							   	(no word order)
>
> One-hot encoding 				 	 	 (space-inefficient; curse of dimensionality)
>
> Word2Vec								 		 (context-independent)
>
> BERT embedding 							 (compute-intensive)
>
> **<u>Word2Vec vs BERT</u>**
>
> \vert  Word2Vec                  \vert  Bert                        \vert 
> \vert  -----------------------------\vert  --------------------------- \vert 
> \vert  context-independent                   \vert  context-dependent           \vert 
> \vert  No word ordering                      \vert  Explicit index position     \vert 
> \vert  Input is single word                  \vert  Input is a sentence         \vert 
> \vert  Cannot handle Out-of-Vocabulary (OOV) \vert  Use WordPiece to handle OOV \vert 

# Statistical MT

 [lecture16_statistical_mt.pdf](../slides/lecture16_statistical_mt.pdf) 

## MT Challenge

**Differences in Syntax & Morphology**

- sentence **word order**
  - SVO: English, Mandarin
  - VSO: Irish, Classical Arabic
  - SOV: Hindi, Japanese

- word order in phrases (adjective modifiers)

- prepositions and case marking

**Lexical Divergence**

<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 6.41.36 PM.png" alt="Screen Shot 2022-08-01 at 6.41.36 PM" style="zoom:50%;" />

### Vauquois Triangle

<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 6.44.33 PM.png" alt="Screen Shot 2022-08-01 at 6.44.33 PM" style="zoom:50%;" />

neural MT: source -> target

phrase-based MT

syntax-based MT(tree to tree)

syntax-based MT(tree to string)

### Faithfulness and Fluency

Good translation needs to be:

- **Faithful**: Target sentence should have the same *content* as the source text.
  - can use any mono-lingual language model
- **Fluent**: Target text should be grammatical/natural/fluent in the target language
  - how would you model faithfulness
  - not always need to...

### MT as Decipherment

Noisy Channel Model for MT

<img src="NLP 0801.assets/Screen Shot 2022-08-01 at 6.52.37 PM.png" alt="Screen Shot 2022-08-01 at 6.52.37 PM" style="zoom:50%;" />

- **Goal**:Given an observation in the **source language** (F), figure out what was said in the **target language** (E).

- Fluency is modeled by $P(E)$.   

  Faithfulness is modeled by $P(F\vert E)$.

- Apply Baye’s Rule $P(E\vert F) \propto P(E) P(F\vert E)$



## Word Alignments

 [Ungraded Exercise: IBM Model 2](https://courseworks2.columbia.edu/courses/153361/assignments/875771?module_item_id=1820660) 

### IBM Model 2

**Simplifying assumption**:  alignments are **one-to-many**, i.e.  each *f* word originates from exactly one *e* word (not many-to-one, many-to-many, ...) 

<img src="NLP 0801.assets/Screen Shot 2022-08-10 at 10.14.00 AM.png" alt="Screen Shot 2022-08-10 at 10.14.00 AM" style="zoom:50%;" />

- alignment variable $a_1...a_m$ to indicate the position that each word $f$ is aligned to (trace-back)

  > $a_1, ..., a_9 = 〈1, 2, 4, 4, 4, 0, 5, 7, 6〉$

define a "**mini translation model": **
$$
P(F\vert E) = \sum_A P(F,A\vert E)
$$
decoding problem: find $\displaystyle \arg\max_F P(F)P(E\vert F)$

we want to model the conditional probability:
$$
\begin{align}
P(F,A\vert E) = & P(f_1...f_m,a_1...a_m\vert e_1...e_l,m)\\
= & \prod_{i=1}^m q(a_i\vert i,l,m)t(f_i\vert e_{a_i})
\end{align}
$$
**parameters: **

- $t(f\vert e)$- **transition parameter:** probability of generating word $f$ from $e$

  > $t(f\vert e) = \frac{count(e,f)}{count(e)}$

- $q(j\vert i,l,m)$ - **alignment parameter: ** probability that alignment variable i takes value j

  > $q(j\vert i,l,m) = \frac{count(j\vert i,l,m)}{count(i,l,m)}$,  $eg: q(1_{Mary}\vert 1_{Maria},7,9)$

**independence assumption: **

- $a_i$ pairwise independent

**computing optimal alignments: **
$$
\arg\max_{a_1...a_m}P(a_1...a_m\vert f_1...f_m,e_1...e_l,m)\\
a_i = \arg\max_{j\in\set{0...l}} q(j\vert i,l,m)t(f_i\vert e_j) \ \ for  \ i=1...m
$$






## MT Evaluation

#### BLUE Metric

- **B**i**L**ingual **E**valuation **U**nderstudy

- Modified n-gram precision with length penalty. Recall is ignored.

- pros: Quick, inexpensive, and language independent; correlates highly with human eval

- cons:  

  - Bias against synonyms and inflectional variations. 

  - Penalizes variations in word-order between languages in different families.

    <img src="NLP 0801.assets/Screen Shot 2022-08-03 at 5.24.59 PM.png" alt="Screen Shot 2022-08-03 at 5.24.59 PM" style="zoom:50%;" />

- BLEU is precision based. Dropped words are not penalized (“colorless” has score 1)
- $$[formula]$$

