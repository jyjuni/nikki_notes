# RNN

 [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 

A recurrent neural network can be thought of as **multiple copies of the *same* network**, each passing a *different* message to a successor.

<img src="rnn_lstm.assets/RNN-unrolled.png" alt="An unrolled recurrent neural network." style="zoom:50%;" />

- å¯¹äºåªéœ€åˆ©ç”¨è¿‘è·ç¦»ä¿¡æ¯çš„çŸ­æœŸè®°å¿†ä»»åŠ¡æœ‰æ•ˆ

é—®é¢˜ï¼š

- åªæœ‰çŸ­æœŸè®°å¿†ï¼šæ¢¯åº¦**è¢«è¿‘æœŸè·ç¦»ä¸»å¯¼**ï¼Œå¯¼è‡´æ¨¡å‹å¾ˆéš¾å­¦ä¹ åˆ°è¿œè·ç¦»çš„ä¾èµ–å…³ç³»ã€‚
- æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸

## RNNè®­ç»ƒ

#### BPTT(Backprop through time)

åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å°†RNNå±•å¼€(ä¸è®°å¿†ä¹‹å‰çš„æ¨¡å‹æƒé‡ï¼Œå› æ­¤æƒé‡å‚æ•°å…¨éƒ¨éƒ½æ˜¯ä¸€æ ·çš„ï¼‰ï¼Œæ¢¯åº¦ä»æœ€è¿‘çš„è®¡ç®—æ­¥éª¤åå‘ä¼ æ’­è‡³ä¹‹å‰çš„è®¡ç®—æ­¥éª¤ï¼Œå†ä¾æ¬¡ä¼ åˆ°ä¸Šä¸€æ—¶é—´æ­¥çš„æ‰€æœ‰è®¡ç®—æ­¥éª¤...



## æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜

*During the backpropagation in the deep neural networks, the Vanishing gradient problem **occurs due to the sigmoid and tan activation function** and the exploding gradient problem **occurs due to large weights.***

â€ä¸€é”…è€é¼ å±â€œï¼šBPæ˜¯é“¾å¼æ³•åˆ™ï¼ˆæ¢¯åº¦ç›¸ä¹˜ï¼‰ã€‚åªè¦ä¸€ä¸ªä¹˜æ•°å‡ºé—®é¢˜å°±ä¼šåäº‹ï¼š

- æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜å‡ºåœ¨**sigmoid/tanh activation**

- æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜å‡ºåœ¨**æƒé‡å‚æ•°è¿‡å¤§**

![Derivation: Derivatives for Common Neural Network Activation Functions -  The Clever Machine](rnn_lstm.assets/common_activation_functions.png)

### åŸå› 

1. å±•å¼€çš„RNNé€šå¸¸éå¸¸éå¸¸æ·±

2. RNNåŒ…å«è®¸å¤šç›¸åŒçš„æ¢¯åº¦é¡¹:

   > non-recurrentï¼š
   >
   > $w_1Â·\alpha_1Â·ğ‘¤_2Â·\alpha_2Â·Â·Â·ğ‘¤_ğ‘‘Â·\alpha_ğ‘‘$
   >
   > RNNï¼š
   >
   > $wÂ·\alpha_1Â·ğ‘¤Â·\alpha_2Â·Â·Â·ğ‘¤Â·\alpha_ğ‘‘$

### ç­–ç•¥

#### å…¶ä»–æ¿€æ´»å‡½æ•°ï¼ˆreluï¼‰

#### Batch Normalization

sigmoidã€tanhè¿™ç±»æ¿€æ´»å‡½æ•°å‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„æ ¹æœ¬åŸå› æ˜¯æ•°å€¼è¿‡å¤§(>=5)ï¼Œæ ‡å‡†åŒ–æ•°æ®å¯ä»¥æŠŠå¤§éƒ¨åˆ†çš„æ•°æ®èŒƒå›´é™å®šåœ¨[-4,4]ä¹‹é—´ï¼Œå› æ­¤å¯ä»¥å¾ˆå¤§ç¨‹åº¦ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

<img src="https://miro.medium.com/max/1400/1*XCtAytGsbhRQnu-x7Ynr0Q.png" alt="img" style="zoom:50%;" />

#### Residual Connections

#### Truncated BPTT

#### gradient clipping



# LSTM

- è§£å†³RNNä¸æ“…é•¿é•¿æœŸè®°å¿†çš„é—®é¢˜ï¼Œ**æ”¹å–„**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

- *åŸå§‹çš„paperé‡Œæ²¡æœ‰forget gate(f_t=1)*



<img src="rnn_lstm.assets/LSTM3-SimpleRNN.png" alt="img" style="zoom:50%;" />

The repeating module in a standard RNN contains **a single layer**.

<img src="rnn_lstm.assets/LSTM3-chain.png" alt="A LSTM neural network." style="zoom:50%;" />

The repeating module in an LSTM contains ***four* interacting layers**.

## ç»“æ„

### cell state(ç»†èƒçŠ¶æ€)

acts as a transport highway that transfers relative information all the way down to the sequence chain

**(think of this as the *memory* of the network)**

### hidden state

short-term memory (ä¸Šä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡º)

**cell state: é•¿æœŸ**

**hidden stateï¼šçŸ­æœŸ**

### gates

![img](rnn_lstm.assets/LSTM2-notation.png)

æ¯ä¸ªgateçš„ç»„æˆï¼š

- **sigmoid NN**ï¼šå†³å®šç•™å¤šå°‘(output [0,1] *for each number* in ...)
- **element-wise multiplication**: ç”¨äºelement-wiseæ§åˆ¶ä¿¡å·æµé€šï¼Œ0-ä¸é€šè¿‡ï¼Œ1-å…¨éƒ¨é€šè¿‡

#### forget gate

![img](rnn_lstm.assets/LSTM3-focus-f.png)

#### input gate 

![img](rnn_lstm.assets/LSTM3-focus-i.png)

#### combining f+i

![img](rnn_lstm.assets/LSTM3-focus-C.png)

#### outputgate

<img src="rnn_lstm.assets/LSTM3-focus-o.png" alt="img" style="zoom:36%;" />



### activations

- æ¿€æ´»ç”¨äºé¢„æµ‹çš„è¾“å…¥ç›¸å…³çš„ç”¨tanh
  - ==åŸå› ï¼Ÿ==

- æ¿€æ´»æ§åˆ¶é—¨ç³»æ•°ç”¨sigmoid
  - åŸå› ï¼šå€¼åŸŸ[0,1]

### å…³äºLSTMæ”¹å–„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ 

[How LSTM networks solve the problem of vanishing gradients - by Nir Arbel - DataDrivenInvestor](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577) 

åŸç†ï¼š

- **the gradient contains the forget gateâ€™s vector of activations, which allows the network to better control the gradients values, at each time step, using suitable parameter updates of the forget gateï¼š** é—å¿˜é—¨$c_t = c_{t-1} * f_t + \tilde c_t * i_t$ å¯ä»¥æ§åˆ¶cell stateä¿ç•™è¿œè·ç¦»çš„æ¢¯åº¦ï¼Œç›¸å½“äºåœ¨cell stateè¿™æ¡â€œé«˜é€Ÿå…¬è·¯â€œä¸Šçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œè¿™é‡Œç±»ä¼¼äºä¸€ä¸ªæ®‹å·®è¿æ¥çš„ä½œç”¨ã€‚
- **cell state gradient is an additive functionï¼š**cell stateçš„æ¢¯åº¦æ˜¯åŠ æ€§æ¨¡å‹ï¼Œæ¶ˆå¤±æ¢¯åº¦+æ­£å¸¸æ¢¯åº¦=æ­£å¸¸æ¢¯åº¦ï¼Œå› æ­¤åªè¦è‡³å°‘ä¸€æ¡è·¯å¾„ï¼ˆé—å¿˜é—¨ï¼‰ä¸Šçš„æ¢¯åº¦ä¿æŒæ­£å¸¸ï¼Œæ€»æ¢¯åº¦å°±å¯ä»¥ç»§ç»­ä¼ é€’ã€‚è€Œæ¯æ¡è·¯å¾„çš„æ¨¡å‹æƒé‡å’Œè®¡ç®—æœºåˆ¶ä¸åŒï¼Œä¸åŒè·¯å¾„çš„æ¢¯åº¦è¡¨ç°å·®å¼‚è¾ƒå¤§ï¼Œå‡ä½äº†æ¢¯åº¦æ¶ˆå¤±çš„å¯èƒ½æ€§ã€‚è€Œä¼ ç»ŸRNNåªæœ‰ä¸€æ¡è·¯å¾„ï¼Œå³ä¸€ä¸ªæ€»æ¢¯åº¦ï¼Œå› æ­¤éš¾ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

### å¦‚ä½•ä¼˜åŒ–

 [GAï¼ˆé—ä¼ ç®—æ³•ï¼‰ä¼˜åŒ–LSTMç¥ç»ç½‘ç»œ-CSDNåšå®¢](https://blog.csdn.net/Vertira/article/details/122403571) 

 [Choosing the right Hyperparameters for a simple LSTM using Keras - Towards Data Science](https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046) 

 [LSTM å¦‚ä½•ä¼˜åŒ–? - çŸ¥ä¹](https://www.zhihu.com/question/449389956) 



-----

 [Illustrated Guide to LSTM's and GRU's: A step by step explanation - YouTube](https://www.youtube.com/watch?v=8HyCNIVRbSU) 

 [Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) 

# GRU

è½»é‡çº§çš„LSTMå˜ç§

ä½¿ç”¨hidden stateæ›¿ä»£cell stateçš„ä½œç”¨ï¼Œhidden stateé‡Œå­˜å‚¨é•¿æœŸ+çŸ­æœŸçš„è®°å¿†ä¿¡æ¯

<img src="rnn_lstm.assets/LSTM3-var-GRU.png" alt="A gated recurrent unit neural network." style="zoom:50%;" />

reset gate: forget some of previous hidden state

update gate: combine previous hidden state + current hidden state(output)



 [Illustrated Guide to LSTM's and GRU's: A step by step explanation - YouTube](https://www.youtube.com/watch?v=8HyCNIVRbSU) 

 [Understanding GRU Networks - Towards Data Science](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) 





## encoder-decoder

encoder: input -> feature vector (feature representations)

decoder: feature vector -> output

*Simple terms , **ENCODER** folds the data to retain imp information and **DECODER** does the final task.

### training

The encoders are trained with the decoders. There are no labels (hence *unsupervised*). The loss function is based on computing the delta **between the actual and reconstructed input**. The optimizer will try to train both encoder and decoder to lower this reconstruction loss.

Once trained, the encoder will gives feature vector for input that can be use by decoder to construct the input with the features that matter the most to make the reconstructed input recognizable as the actual input.

It is important to know that in actual application, people donot try to reconstruct the actual input, but rather want to map/translate/associate inputs to certain outputs. For example translating french to english sentences, etc.

[What is an Encoder/Decoder in Deep Learning? - Quora](https://www.quora.com/What-is-an-Encoder-Decoder-in-Deep-Learning)  

[What is an encoder decoder model? - by Nechu BM - Towards Data Science](https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a) 

